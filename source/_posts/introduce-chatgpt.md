---
abbrlink: ''
categories:
- - 科技
- - chatgpt
- - AI
- - 人工智能
date: '2024-07-23T19:44:18.689508+08:00'
tags:
- 科技
title: 关于Chatgpt的一篇演讲稿
updated: '2024-07-23T19:44:31.437+08:00'
---
# 来源

本来是要在班里演讲，因为期末考试被迫停止QAQ
留个纪念（

# 正文

有人说，2023年是人工智能元年。因为OpenAI公司在2022年11月30日公开发布了ChatGPT。从此，人们得以通过与ChatGPT对话的形式，直接了解到人工智能带给我们的便利。而如今已是2024年，相信大家对Chatgpt都有了或多或少的了解，或是从网络媒体的报道，或是从语文作文，抑或是从英语阅读中。但大家是否想对其有更深入一点的理解呢？今天就让我们从chatgpt出发，了解其背后的原理，在进一步去探索一下背后的神经网络的基本模式。

让我们先从Chatgpt的名称开始说起吧。Chatgpt全称：Chat Generative Pre-trained Transformer，意为生成式预训练Transformer。生成式，即指Chatgpt是一个用来生成新文本的机器人。“预训练”，指模型经历了从大量数据中学习的过程。而关键是最后一个词，transformer，这是什么？其实他是一种特殊的一种深度学习模型，一个神经网络。是Google在2017年发布的一篇论文《Attention Is All You Need》中提出的一个架构。Chatgpt就是基于Transformer模型进一步修改得来的。

那么深度学习是什么意思呢？深度学习是机器学习的一种，而机器学习采用数据驱动，反馈到模型参数，指导模型行为。可能有点难懂啊。这里的模型，其就是一个广义上的函数，即映射。而神经网络指的是这个函数内部的实现方式。一会会介绍。继续说机器学习。比如说，一个可以给图片加标签的函数，fx，输入x是图片，而输出fx即为标签。机器学习的核心理念就是，不要在代码中定义任何行为，而是去构造一个有可调参数的灵活函数，然后拿着大量的例子，去让机器调整各个参数，以模仿这种表现。其实有点像数学里的待定系数法。

了解了Transformer模型，我们再来看简单地看一下Chatgpt的原理。如果用一句话去概括，就是：以前面所有的文本为基础，预测出现频率最高的下一个词，来实现文本生成的。其实有点像搜索引擎的自动补全，我们每输入一个词，输入框就开始预测后面的文本，概率越高的排在越上面。但模型是如何的到每个词出现的概率的呢？

这就提到了上面所说的Transformer架构了。上面说到了，其实Transformer就是一个函数。如果拿f(x) 举例，他的输入值x，即为前面所有的文本，输出值即为每一个词的概率。我们都知道，函数，例如二次函数f(x) = ax^2+bx+c，abc就是这个函数的参数，对于这个函数来说，参数数量是3。那么，我们自然想问，对于ChatGpt的修改后的Transformer架构，他的参数数量是多少呢？大家可以猜一猜。答案是1750亿个。

为什么会有这么多参数，又如何让机器去调整呢？下面，我们用一个简单一点的例子，数字识别来理解一下神经网络。就是这个函数的内部实现机制。

首先，我们必须承认一个事实，其实让计算机识别数字是很困难的。你的3和我的3可能大相径庭。科学家们发明出了一套神奇的东西，即神经网络去解决这个问题。

顾名思义，神经网络来源于人的大脑结构。神经中，最重要的就是神经元和神经元的连接方式。神经元可以理解为装有数字的容器，装着从0—1的数字，称为激活值。对于识别数字这个任务来说，输入的每一个神经元对应了图上的每一个像素的颜色深浅。拿28像素x28像素的图像举例，输入的神经元就有784个，构成了神经网络的第一层。接着跳到最后一层，有十个神经元，代表0—9十个数字，他们的激活值也在0—1之间。这些值代表系统认为输入的图像对应 此数字的可能性。网络中间还有隐含层，里面进行处理识别数字的具体工作。

神经网络运行时候，上一层的激活值决定了下一层的激活值。所以说，神经网络的核心就是上一层的激活值如何算出下一层的激活值。其实，设计出的神经网络想要模仿生物的神经系统，某些神经元的激发，会导致其他神经元激发。

第一层的激活值会算出第二层每个神经元的激活值，依次往下，直到最后，输出层里面最亮的就是神经系统的选择。所以为什么要分层？我们其实期待每一层可以特异性识别某些特征。。。每一层的激活值如何算出下一层的激活值靠的就是“参数”，比如第一层784个神经元的激活值是a1,a2,a3,…,a784。那么假设第二层有16个神经元，每一个bi，要想由第一层算出来，就要784的参数，例如b1=w1\*a1+w2\*a2+…+w784\*a784。一开始这些值都是随机的，也别指望电脑能识别出什么东西。我们要做的就是拿出大量的已经写好的数字图片，和对应的数字，每次都“投喂”给电脑，并算出电脑做的有多“差”——代价函数，并给出修改参数的“意见”——代价函数的负梯度。经过一次的修改，这些参数一次次的调整，之后遇到新的图片，神经网络也可以应付了。

其实，对于我们日常学习来说，有何尝不是如此呢。及时对自己表现评估。找到最“高效”的提升方法加上大量练习，就会一定会自己学习得更好。+补充写一些QAQ

